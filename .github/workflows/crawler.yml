name: Hot News Crawler

on:
  schedule:
    - cron: "0 * * * *"
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages-${{ github.ref }}"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true

      - name: Run crawler
        env:
          RUN_MODE: "once"
        run: |
          python main.py
          test -f output/latest.html || echo '<h1>Pending data</h1>' > output/latest.html
          echo '<meta http-equiv="refresh" content="0;url=latest.html">' > output/index.html

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: output

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
